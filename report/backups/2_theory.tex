\chapter{Theory} \label{ch:theory}
In order to fully understand VAEs we first need to explore and understand Autoencoders and Variational Inference, as these two settings provide the basis for VAEs. 
\section{Autoencoders}
An autoencoder is a neural network that has the capability to copy the input to its output \cite[Chp.~14]{Goodfellow-et-al-2016}. Suppose we are given some input $\x = \{x_1, ..., x_N\}$ of a training data set that we input map to the code $\z$ that represents $\x$, which in turn is mapped to the reconstructed output $\r$. The generation of $\z$ is produced by an encoder function $f(\x)$ and $\r$ is produced by a decoder function $g(\z)$, hence we get the identity function $\r = f(g(\x))$ \cite[Chp.~14]{Goodfellow-et-al-2016}. An autoencoder is composed of these two parts. Each component can be seen as simple feed-forward neural networks with each their respective sets of weights $\theta$ and $\phi$  \cite[Chp.~14]{Goodfellow-et-al-2016}. On these weights, we can apply any gradient descent method to train their respective weights \cite[Chp.~14]{Goodfellow-et-al-2016}. More formally, the components encompass the conditioned mappings $\pencoder(\z|\x)$ and $\pdecoder(\x|\z)$ \cite[Chp.~14]{Goodfellow-et-al-2016}. A fully optimized vanilla autoencoder is not interesting, because these do only reconstruct the input \cite[Chp.~14]{Goodfellow-et-al-2016}. \\

On the contrary, we are interested in learning to capture useful features of the training data in the encoder. This has the advantage of improving the computational speed and space capacity. This can be done by using an undercomplete autoencoder where its code dimension is lower than the input dimension. The learning process then attempts to preserve the most salient features represented by the data distribution $\z$ \cite[Chp.~14]{Goodfellow-et-al-2016}. The number of dimensions of $\z$ would then be a hyperparameter, though the output $\r$ would keep the input dimension. In figure XXXX (TODO) we can see the architecture of an undercomplete autoencoder.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{images/autoencoder}
    \caption{TODO}
    \label{fig:autoencoder}
\end{figure}
The learning process is to minimize the following objective
\begin{align} \label{eq:1}
\L(\x, \r)
\end{align}
which penalizes the difference between the input and output of our reconstruction, hence we identify it as a reconstruction loss \cite[Chp.~14]{Goodfellow-et-al-2016}. This loss function $\L$ can e.g. be represented by a $L^1$ or $L^2$ loss. \\
It is easy to see that such an autoencoder with a linear encoder and decoder, where $\L = L^2$, it is essentially a PCA that spans the same subspace of the training data \textbf{TODO} \cite[Chp.~14]{Goodfellow-et-al-2016}.

\begin{itemize}
    \item Sparse autoencoders are an extension of the classical autoencoders as it adds an additional term $\Omega({\textbf{z}})$ to the objective function. This term works as a regularizer term that adds sparsity to thye model which again may result in learning new features as a side-effect. This can be used as framework to approximate maximum likelihood training of a generative model that has latent variables $\textbf{z}$. \cite[Chp.~14]{Goodfellow-et-al-2016}.
    \item $x$ is visible variable, $z$ is latent variable. $p(z)$ is the model's prior distribution over the latent variables.
    \item we have join distribution $p(x,z) = p(z)p(x|z)$
    \item log-likelighood can be decomposed as $log p(x) = log \sum_h p(z,x)$ where the autoencoder approximates this sum \cite[Chp.~14]{Goodfellow-et-al-2016}.Hence, we are interested in maximizing $log p(h,x) = log p(h) + log p (x|h)$. 
    \item Stochastic autoencoders: Generalize the notion of an encoding function $f(x)$ to an encoding distribution $p_{encoder}(z|x)$ where the latent variable model $p_{model}(z,x)$ is defined as $p_{encoder}(h|x)=p_{model}(z|x)$ and a stochastic decoder $p_{decoder}(x|h) = p_{model}(x|h)$.
    \item Autoencoders learn the structure of manifold that is given a set of its tangent planes, where a point x on a $d$- dimensional manifold, the tangent plance is given by $d$ basis vectors \cite[Chp.~14]{Goodfellow-et-al-2016}. Typically, autoencoders compromise between two forces: (i) Learning a represenation $z$ of a training example $x$ such that $x$ can be approximately recovered from $z$ through a decoder. (ii). Satisfying the regulaization penaly, often added to the reconstruction loss. we try to balance these two opposing forces. \cite[Chp.~14]{Goodfellow-et-al-2016}
    \item We aim to use a model that combines these properties as that will make the hidden representaiton to captue information about the structure of the data-generating distribution \cite[Chp.~14]{Goodfellow-et-al-2016} (that will prefer solutions thatr are less sensitive to the input). The important principle is that the autoencoder can afford to represent only the variation that are needed to reconstruct training examples \cite[Chp.~14]{Goodfellow-et-al-2016}
    \item Only the variation tangent to the manifold around x need to correspond to change in $z = f(x)$, hence the cnsoder learns a mapping from the input space $x$ to a represenation space, a mapping that is only sensistive to changes along the manifold directions, but that is insensitive to changes orthogonal to the manifold. This can actually if the data-generating distritubiont concentrates near a low-dimensional manifold which invokes represenation that implicityly captur a local coordinate system for this manifold. \cite[Chp.~14]{Goodfellow-et-al-2016}
    \item Problem with sparse and denoise: Can't sample directly from $p(x)$. 
\end{itemize}

\section{Variational Inference}
Variational inference (VI) is a method where we aim to compute latent representations of our given observation $\x$, i.e. variables that explain the input approximately. These representations are inferred given $\x$. The latent variables can be either discrete or continuous, but we will strictly consider the latter. 

\begin{itemize}
    \item exact inference would be intractable to compute, hence we need to approximate the posterior instead. 
    \item Important that $\D_{KL}(q(z|x)||p(z|x))$ is not symmetric.
    \item minimize $J(\theta)$ w/ $\theta \in \R^n$ by using multivariate calculus $\triangledown_{\theta}J(\theta) = 0$
\end{itemize}
\begin{itemize}
    \item All from \cite[Chp.~19]{Goodfellow-et-al-2016}
    \item We want to compute log probability of the observed data, $log \; p(x;\theta)$, but is often intractable. Instead, we compute the lower bound $\L(x,\theta, q)$ on $log\;p(x;\theta)$. This bound is called evidence lower bound (ELBO). It is defined as $\L(x,\theta,q) = log\;p(x;\theta)-\D_{KL}(q(z|x)||p(z|x;\theta))$, where $q$ is an arbitrary probability distribution over $z$.
    \item Because a KL divergence is non-negative $\L$ has a most the same values as the desired log-probability. The divergence measures the similarity between two probability distributions, converging to 0 if similar.
    \item ELBO can be derived to be $\L(x, \theta, q) = \E_{h \sim q}[log \; q(z|x) - log \; p(z,x;\theta)]$.
    \item for $q(z|x)$ that are better approximations of $p(z|x)$ the lower bound $\L$ will be tighter, i.e. closer to $log \; p(x)$. When $q(z|x) = p(z|x)$ the approximation is perfect and we have $\L(x,\theta,q) = log \;p(x;\theta)$
    \item in other words, inference is a procedure that finds $q$ that maximizes $\L$ w.r.t $\theta$. An exact inference would search over a family of functions $q$ that includes $p(z|x)$.
    \item we can also think of maximizing $\L$ w.r.t $q$ as minimizing $\D_{KL}(q(z|x)||p(z|x))$, meanining we fit $q$ to $p$, In otherw words, when we use maximum likelihood learning to fit a model to data, we minimize $\D_{KL}(p_{data}||p_{model})$ which means that maximum likelihood encourages the model to have high probability everywhere that the data has high probability, and $q$ is encouraged to have low prob everywhere the true posterior has low probability (so two directions)
    \item algorithm to maximize a lower bound $\L$ is expectation maximization (EM). Is not really approximate inference but an approach to learn with an approximate posterior.
    \item Computing $\D_{KL}(p(z|x)||q(z|x))$ would require computing expectations w.r.t to the true posterior $p(z|x)$ which again is intractable.
    \item When training the parameters, variational learning increases $\E_{z \sim q}log\; p(x,z)$. FOr a specific $x$, this increases $p(z|x)$ for  values of $z$ that have high probability under $q(z|x)$ and decreases $p(z|x)$ for values of $z$ that have low probability under $q(z|x)$.
    \item we often estimate $log \; p(x;\theta)$ after training the model and find that the gap with $\L(x,\theta, q)$ is small. From this, we can conclude that our variational approximation is accurate for the specific value of $\theta$ that we obtained from the learning process. We want $\theta^* = max_{\theta}log \; p(x;\theta)$. 
    \item In VAE we have learned approximate inference uses for generative modeling. In VAE we don't need to construct targets for the inference network, instead, the network is simply used to defined $\L$, and then the parameters of the inference network are adapted to increase $\L$.
\end{itemize}
\begin{itemize}
    \item from https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf
    \item KL divergence is defined to be $\D_{KL}(q||p) = \E_q[\frac{q(Z)}{p(Z|x)}$. If $q$ and $p$ are high, it's good. If $q$ is high but $p$ is low, we pay a price. If $q$ is low, we don't care because of the expectation. We chose the first to be $q$ because of the expectation. 
    \item By Jensen's inequality we get
    \begin{align}
        \log p(x) = \log \int_z p(x,z) \\
        &= \log \int_z p(x,z)\frac{q(z)}{q(z)} \\
        &= \log (\E_q[p(x,Z)/q(z)]) \geq \E_q[\log p(x,Z)]- \E_q[\log q(Z)]
    \end{align}
    \item Then, we mazimize the ELBO to find the parameters that gives as tight a bound as possible on the marginal probability of $x$. \item Using $p(z|x) = \frac{p(z,x)}{p(x)}$, we get
    \begin{align}
        \D_{KL}[q(z)||p(z|x))] = \E_q[\log \frac{q(Z)}{p(Z|x)}] \\
        &= \E_q[\log q(Z)] - \E_q[\log p(Z|x)] \\
        &= \E_q[\log q(Z)] - \E_q[\log p(Z|x)] \\
        &= \E_q[\log q(Z)] - \E_q[\log p(Z,x)] + \log p(x) \\
        &= -(\E_q[\log p(Z,x)]- \E_q[\log q(Z)]) + \log p(x)
    \end{align}
    which is the negative ELBO plus the log marginal probability of $x$.
    \item $\log p(x)$ does not depend on $q$, so minimizing the KL divergence is the same as maximizing the ELBo.
    \item difference  between ELBO and KL divergence is the log normalizer which is that the ELBO bounds.
\end{itemize}

\begin{itemize}
    \item from \cite{BleiKucukelbircAuliffe}
    \item VI ais a method that approximate probability densities through optimization and tends to be fast as opposed to Markov Chain Monte Carlo sampling. 
    \item Idea: First posit a family of densities and then find the member of that family which is close to the target. Closeness is then measure by KL divergence. 
    \item Consider a joint density of latent variables $\z = z_{1:m}$ and observations $\x = x_{1:n}$ so $p(\z,\x) = p(\z)p(\x|\z)$.
    \item In Bayesian models, the latent variables help govern the distribution of the data. A Bayesian model draws the latent variables from a prior density $p(\z)$ and then relates them to the observations through the likelihood $p(\x | \z)$. Inference in Bayesian model amounts to conditioning on data and computing the posterior $p(\z | \x)$. This oftne requires approximate inference.
    \item for complex models and large data sets, MCMC algorithms are slower than approximate conditional.
    \item In MCMC we sample, in VI we use optimization. We seek to find $q^*(\z) = argmin_{q(z)\in \D} \D_{KL}(q(\z)||p(\z|\x)$. UYsing $q^*(.)$ we approximate the posterior. $\D$ is a family of approximate densitites.
    \item Essentially, MCMC and VI solve the same problem but have different approaches: MCMC sample a Markov chain; variational algorithms solve an optimization problem, and MCMC algorithms approximate the posterior with samples from the chains, whereas variational algorihtm approximate it wit hthe result of the optimization.
    \items VI find a density close to the target, so it apprioximates densities. MCMC simulates from densitites.
    \item Goal of VI is to approximate a conditional density of latent variables given observed variables. Key idea is to solve this problem with optimization. We use a family of densities over the latent variables, parametrized by free variational parameters. The optimization find the member of this family, i.e., the setting of the parameters, that is closed in KL divergence to the conditional of interest. 
    \item Inference problem is to comptue the conditional density of the latent variables given the observations, $p(z|x) = \frac{p(z,x)}{p(x)}$. Denominator has the marginal density of the oberservations, and is computed by marginalizing out the latent variables from the joint density, $p(x) = \int p(z,x)dz$. Is intractable - requires exponential time to compute or even unavailable in closed form..  
    \item In VI we specify a family $\D$ if densities over the latent variables. Each $q(\z) \in \D$ is a candidate approximation to the exact conditional. By using KL divergence, we find the best candidate that is lowest in KL divergence to the exact conditional.
    \item KL divergence is $KL(q(z)||p(z|x) = \E[log q(z)] - \E[\log p(z|x)]$ where expectations are tkaen w.r.t to $q(z)$. Exanding the conditional we get $KL(q(z)||p(z|x) = \E[log q(z)] - \E[\log p(z,x)] + \log p(x)$.
    \item KL cant we computed because of $\E[log p(z|x)]$ that requires $p(x)$, thus we optimize an alternative objective that is equivalent to the KL up to an added constant:
    $ELBO(q) = \E[logp(z,x)] - \E[log q(z)]$ which is the negative KL divergence plus $\log p(x)$ whic his a constant w.r.t $q(z)$. Maximizing the ELBO is equivalent to minizming the KL divergence.
    \item $\forall p,q \; \D_{KL}(q||p) \geq 0$. 
    \item $\log p(x) = KL(q||p) + ELBO(q)$ and of course $\log p(x) \geq ELBO(q)$. 
    \item problem is now reduced to maximizing ELBO(q) over all distributions belonging to $\D$. 
    \item approximation to $p(z|x)$ is $q^*(\z)$ which is a distribution over $\z$.
    \item rewrite the ELBO as a sum of the expected log likelihood of the data and the KL divergence between the prior $p(z)$ and $q(z)$: $ELBO(q) = \E[log p(z)] + \E[log p(x|z)] - \E[log q(z)] = \E[log p(x|z)] - \D_{KL}[q(z)||p(z)]$
    \item first term is an expected likelihood; it encourages densitites that place their mass on configurations of the latent variabales that explain the observed data. Second term is the negative divergence between the variational density and the prior; it encourages densities lose to the prior. So balance between similarity to likelihood and prior.
    \item ELBO lower bounds the log evidence so $log p(x) \geq ELBO(q)$ for any $q(z)$.
    \item hence, we get $log p(x) = KL(q(z)||p(z|x)) + ELBO(q)$.
    \item when ELBO is equal to the log likelihood $\log p(x)$ we have $q(z) = p(z|x)$ becausde then the KL divergence is 0. 
\end{itemize}

\begin{itemize}
    \item All from \cite[Chp.~16]{Goodfellow-et-al-2016}
    \item A structured probabilistic model is a way of describing a probability distribution using a graph to describe which random variables in the probability distribution interact with each other directly.
    \item Deep learning is a subfield in machine learning that excels in scaling to solve challenges in AI that have rich structures in high dimensions. For instance, a domain would be to understand natural images, speech recognition and so on.
    \item classifiers discard most of the information in the input and produces a single output, i.e. a probability distribution over values of that single output. The classifier is also often able to ignore many parts of the input.
    \item probabilistic models often require a complete understanding of the entire structure of the input, with no option to ignore sections of it.
    \item Problem: model a distribution over a random vector x vcontaining $n$ discrete variables capable of taking on $k$ values each, then a naive approach of representing $P(x)$ by storing a lookup table with one probability value per possible outcome requires $k^n$ parameters. Too costly in terms of computation and storing. of we would want to do inference or sampling.
    \item Structured probabilistic models provide a formal framework for modeling only direct interactions between random variables. Allows models to have significantly fewer parameters and therefore be estimated realiably from less data.
    \item vertices represent R.Vs. Edges represent interactions between random variables. 
    \item a -> b means the distribution b depends on the value of a. So p(b|a)
    \item in a latent variable model we might want to extract features $\E[z|x]$ describing the observed variables $x$. We often train out models using the principle of maximum likelihood because $log \; p(x) = \E_{z \sim p(z|x)}[log \; p(z,x) - log \;p(z|x)]$ and we often want to compute $p(z|x)$ in order to implement a learning rule. 
    \item Inference problem: We must predict the value of some variables given other variables or predict the probability distribution over some variables gven the value of other variables. However, intractable to compute, hence we use approximate inference, where we approximate the true distribution $p(z|x)$ by seeking an approximate distribution $q(z|x)$ that is as close to the true one as possible.
\end{itemize}

\section{Variational Autoencoders}
\begin{itemize}
    \item Now that we have acquanted ourselved with the basis/frameworks of Autoencoders and Variational Inference we now arrive at Variational Autoencoders (VAE).
    \item Theoretical connection between autoencoders, laten varialbes as seen in VI have brought VAEs
    \item Compared to AE, here we approximate the maximium of the training data instead co copying input to the output \cite{Goodfellow-et-al-2016}.
    \item $\sigma^2$ is noise variance
    \item Learning latent representation was already inspected with earlier works such as denoising autoencoder, sparse autoencoder.
    \item reconstruction error only would encourage a VAE to learn an identity function. The D KL divergences encourages the VAE to learn to similar data to $x$ from the latent space that deviates by $\sigma$(?).
    \item we maximize a lower bound on the log-likelihood of the model. 
    \item VAE: given probabilistic distribution model we can say that x' only indirectly is affected by x.
    \item so have like $p(x,z,x') = p(x)p(z|x)p(x'|z)$
    \item it is intractable to integrate over all possible join assignments of the input $x$.
\end{itemize}

\begin{itemize}
    \item explain why VAE and not AE, GAN, what the advantage and disadvantage of VAE is
    \item derive the math 
    \item explain VI
    \item explain what isotropic Gaussian means. Maybe even plot it? And explain why that is an interesting property.
    \item applications: rrecognition, denoising, representation and visualization purposes \cite{KingmaWelling}
\end{itemize}

\begin{itemize}
    \item from \cite{KingmaWelling}
    \item the posterior is intractable. 
    \item variational lower bound yields a simple differentiable unbiased estimator of the lowwer bound. 
    \item apprixmate posterior inference in almost any model w/ continuous latent variables and/or parameters and can be optimized using standard gradient descent.
    \item VAE make inference learning with the SGCB estimator to optmize an endoer/recognition model using simple ancestral sampling, which makes it possible to learn the parameters efficiently.
    \item Suppose, we are given an i.i.d. dataset with dataset with latent variables per datapoints, and we like to perform maximum likelihood inference on the parameters and variational inference on the latent variables.
    \item the algorithm should work in general even if we have intractibility and a large dataset.
    \item true posterior $p_{\theta}(\z|\x) ) (p_{\theta}(\x|\z)p_{\theta}(\z))/p_{\theta}(\x)$.
    \item given data set $\x_{1:N}$. 
    \item Solution for 3 probelsm:1. efficient ML for parameters $\theta$ which allows us to genrate data that resemebles the original input. 2. Efficient approximate posterior inference of the latent variable $\z$ given an observed value $\x$ for a choice of parameters $\theta$. 3. Efficient approximate marginal inference of the variable $\x$.
    \item recognition model $q_{\phi}(\z|\x)$, that is an approximation to the intractable true posterior $p_{\theta}(\z|\x)$. The encoder is a probabilistic model, as it given a datapoint $\x$ produces a distribution over th epossible values of the cod $\z$ frin which  the datapoint $\x$ could have been generated. $p_{\theta}(\x|\z)$ is a probabilistic decoder as it given $z$ produces a distribvtuion over the possible corresponding values of $\x$. 
    \item reparam trick: $\z \sim q_{\phi}(\z|\x)$ is some conditional distribution. We express the RV as a deterministic variable $\z = g_{\phi}(\epsilon, \x)$ to construct differentiable estimator of the lariational lower bound. $q$ is often $=\N(\mu, \sigma^2)$, where a vlaid reparam is $z = \mu + \sigma \epsilon$ where $\epsilon$ is an auxiliary noise variable $\epsilon \sim \N(0, 1)$.
    \item need to do reparam so that the  MC estimate of the expectation is diff. w.r.t $\phi$
    \item $q_{\phi} (\z|\x)$ is the approximation to the posterior of the generative model $p_{\theta}(\x, \z)$. prior over the latent variables is isotropic multivariate Gaussian $p_{\theta}(\z)$. $\p(\x|\z)$ is a multivariate Gaussian for real-valued data. The true posterior $\p(z|x)$ is intractable, and assume we have a diagonal covariance structur such that $\log \q (\z|\x) = \log \N(z;\mu^i, \sigma^2 I$, where $\mu$ and $\sigma$ are outputs of the encoder MLP $\q$. 
    \item TODO: Use tutorial to explain more about the reparam trick...
\end{itemize}

\begin{itemize}
    \item from \cite{Doersch}
    \item If the latent streucture helps the model to accurately reproduce  (i.e. maximize the liklihood of) the training set, then the network wil llearn that structure at some layer. 
\end{itemize}

\begin{itemize}
    \item All from \cite[Chp.~20]{Goodfellow-et-al-2016}
    \item VAE is a directed model that uses learned approximate inference and can be trained with gradient-based methods.
    \item To generate a sample from the trained model, the VAE draws a samplke $z$ from the code/latent distribution $p_{model}(z)$. The samp√•le is then run through a differentianle generator network $g(z)$. Then, $x$ is sampled from a distribution $p_{model}(x;g(z)) = p_{model}(x|z)$. The approximate inference network (encoder) $q(z|x)$ is used to obtain $z$ and $p_{model}(x|z)$ is then the decoder network.
    \item We maximize the variational lower bound $\L(q)$ associated with data point $x$: $\L(q) = \E_{z \sim q(z|x)} log \; p_{model}(z,x) + \H(q(z|x)) = \E_{z \sim q(z|x)}log \; p_{model}(x|z) - \D_{KL}(q(z|x)||p_{model}(z)) \leq log\; p_{model}(x)$ (remember the general equation from earlier, I think I forgot to include the H thing as well. First line has as first the join log-likelihood of the visible and hidden variables under the approximate posterior over the latent variables. The second term is the entropy of the approximate posterior. When $q$ is chosen to be a Gaussian distribution, with noise added to a predicted mean value, maximizing this entropy term encourages increasing the standard deviation of this noise. So this entropy term encourages the variational posterior to place high probability mass on many z values that could have generated x, rather than collapsing to a single point estimate of the most likely value. In next line, first term is the reconstruction log likelihood like in other autoencoders. The second terms tries to make the approximate and posterior distribution $q(z|x)$ and the model prior $p_{model}(z)$ approach each other.
    \item VAE train a parametric encoder that produces the parameters of $q$. As long as $z$ is a continuous variable, we can then back-propagate through samples of $z$ drawn from $q(z|x)=q(z;f(x;\theta))$ to obtain a gradient w.r.t $\theta$. Learning then consistes only of maximizing $\L$ w.r.t the parameters of the encoder and decoder.
    \item advantages: nice theory, simple to implement
    \item drawback of VAE: get trained images that are blurry. DL book says not sure why that is - TODO: Find out if recent articles talk about it more?
    \item may be because of Gaussian foir $p_{model}(x;g(z))$ the model assigns high prob to points that ocur in the training set but also other points which may include blurry imnages.
    \item maximizing a lower bound on the likelihood of such a distribution is similar to training a traditional autoencoder with $L^2$ loss, as it has a tendency to ignore features of the input that occupy few pixels or tht cause only a small change in the brightness of the pixels that they occupy.
    \item another disadvg of VAE: It learns an inference network for only one problem, inferring $z$ given $x$. 
    \item nice: VAE increase a bound on the log-likelihood of the model. Also, don't need to restrict the choice of models "generic framework" (my words).
    \item it's a manifold learning algorith, as the encoder and decoder force the model to learn a predictable coordinate system that the encoder can capture.
    \item they also show pictures of z being 2-D which is doable although the instrict dimensionality of the data is much higher. They show examples of images generated by the decoder $p(\x | \z)$.
\end{itemize}

\section{Implementation of a VAE}
\begin{itemize}
    \item Encoders and decoders may be deep, i.e have multiple layers but we will, for the purpose of simplicity, use  2-layer neural networks in each of them. 
    \item According to the universal approximation theorem  this sufficies; a single layer network with a finite number of neurons can approximate any function given a finite number of neurons (TODO: cite here paper). But given we want the latent space be sparse, a deep autoensoder can with an additional hidden layer inside the encoder, approximate any mapping from input to latent space given sufficient number of neurons \cite[Chp.~14]{Goodfellow-et-al-2016}.
\end{itemize}

\subsection{Testing the implementation}

\section{Terms}
\begin{itemize}
    \item $P(X)$: The probability that $X$ is generated in our true distribution. Want to maximize it and $X$ is fixed. But we can only estimate it? With $P(X|z)$?
    \item $P(z)$: Typically $z$ is sampled from standard normal distribution $N(z|0, I)$. Is a prior. Results in very "bad" z's, unlikely to give good X when supplied to $P(X|z)$?
    \item $P(X|z)$: is gaussian. For most $z$, $P(X|z)$ will be nearly 0. Hence, we need $Q(z|X)$. 
    \item $Q(z|X)$: Is a function that takes a value of $X$ and gives us a distribution over $z$ values that are likely to produce $X$. Will hopefully match $P(z|X)$. $Q$ can be any distribution.
    \item $P(z|X) $: is the true posterior
    \item $D[Q(z|X)|||P(z|X)]$ is the KL divergence between the two and works like an error term that we aim to minimize. 
    \item $D[Q(z|X)|||P(z)]$ is a KL-divergence between two multivariate Gaussian distributions.
\end{itemize}

\section{asd}
\subsection{bla}

\begin{subequations}\label{eq:1}
\begin{align} 
 a^2+b^2=c^2\label{eq:1a} \\
 a^2+b^2=c^2\label{eq:1b} 
\end{align}
\end{subequations}
Text \eqref{eq:1a} \autoref{eq:1a} \ref{eq:1a} \autoref{eq:1}

\newpage
